{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff8eca5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "#no need to refresh kernel when changes are made to the helper scripts\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "354864c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16a9b40f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "from IPython.display import display,FileLink, Markdown, HTML, Image\n",
    "\n",
    "cwd = os.path.dirname(os.getcwd()) #add the cwd so that python scripts can be imported.\n",
    "if cwd not in sys.path:\n",
    "    sys.path.insert(0, cwd)\n",
    "\n",
    "#to save in the same directory as the notebook, change to resource_path=\"\".\n",
    "\n",
    "# print(cwd)\n",
    "# print(project_root)\n",
    "# print(resource_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9608ac7e",
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "#parameters\n",
    "gse = \"GSE247175\"\n",
    "project_root = cwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c65c3cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "resource_path = os.path.join(project_root, \"public\", gse) #make sure to replace with output!!!\n",
    "Path(resource_path).mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "946e9a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "import contextlib\n",
    "\n",
    "@contextlib.contextmanager\n",
    "def suppress_output(stdout=True, stderr=True, dest='/dev/null'):\n",
    "    ''' Usage:\n",
    "    with suppress_output():\n",
    "        print('hi')\n",
    "    '''\n",
    "    dev_null = open(dest, 'a')\n",
    "    if stdout:\n",
    "        _stdout = sys.stdout\n",
    "        sys.stdout = dev_null\n",
    "    if stderr:\n",
    "        _stderr = sys.stderr\n",
    "        sys.stderr = dev_null\n",
    "    try:\n",
    "        yield\n",
    "    finally:\n",
    "        if stdout:\n",
    "            sys.stdout = _stdout\n",
    "        if stderr:\n",
    "            sys.stderr = _stderr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f75f0483",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Bio import Entrez\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "os.environ['ENTREZ_EMAIL'] = os.getenv('ENTREZ_EMAIL')\n",
    "\n",
    "Entrez.email = os.environ['ENTREZ_EMAIL']\n",
    "\n",
    "id_handle = Entrez.esearch(db=\"gds\", term=f\"{gse}[Accession]\", retmax=1)\n",
    "id_record = Entrez.read(id_handle)\n",
    "gds_id = id_record[\"IdList\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddf63e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "stream = Entrez.esummary(db='gds', id=gds_id)\n",
    "record = Entrez.read(stream)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55e6e96f",
   "metadata": {},
   "outputs": [],
   "source": [
    "map_species = {\n",
    "    \"homo sapiens\": \"human\",\n",
    "    \"mus musculus\": \"mouse\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44949fc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "species = map_species[record[0]['taxon'].lower()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9738088",
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(record[0]['PubMedIds'])==0: #discard studies with no pubmed citation\n",
    "    raise ValueError(\"No PubMed citation found for this study.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8f934bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "pmid = int(record[0]['PubMedIds'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07b1f071",
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(record[0]['Samples']) not in range(6, 25): #discard studies that dont have 6-24 samples\n",
    "    raise ValueError(\"Number of samples need to be within 6-24.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f98ce279",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_pubmed_metadata(pmid):\n",
    "    handle = Entrez.efetch(db=\"pubmed\", id=pmid, retmode=\"xml\")\n",
    "    records = Entrez.read(handle)\n",
    "    article = records['PubmedArticle'][0]['MedlineCitation']['Article']\n",
    "\n",
    "    title = article['ArticleTitle']\n",
    "    journal = article['Journal']['Title']\n",
    "    journal_abbr = article['Journal']['ISOAbbreviation']\n",
    "    year = article['Journal']['JournalIssue']['PubDate'].get('Year', '')\n",
    "    volume = article['Journal']['JournalIssue'].get('Volume', '')\n",
    "    issue = article['Journal']['JournalIssue'].get('Issue', '')\n",
    "    pages = article.get('Pagination', {}).get('MedlinePgn', '')\n",
    "    authors = article.get('AuthorList', [])\n",
    "\n",
    "    def format_author(author):\n",
    "        initials = ''.join(author.get('Initials', ''))\n",
    "        return f\"{author['LastName']} {initials}\"\n",
    "\n",
    "    authors = [format_author(a) for a in authors]\n",
    "\n",
    "    return {\n",
    "        \"title\": title,\n",
    "        \"journal\": journal_abbr,\n",
    "        \"year\": year,\n",
    "        \"volume\": volume,\n",
    "        \"issue\": issue,\n",
    "        \"pages\": pages,\n",
    "        \"authors\": authors\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a1876f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "pmdict = fetch_pubmed_metadata(pmid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19b00ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_ama_citation(metadata, pmid=pmid):\n",
    "\n",
    "    authors = metadata['authors']\n",
    "    if len(authors) > 6:\n",
    "        author_str = ', '.join([a for a in authors[:6]]) + ', et al'\n",
    "    else:\n",
    "        author_str = ', '.join([a for a in authors])\n",
    "\n",
    "    citation = (\n",
    "        f\"{author_str}. {metadata['title']} \"\n",
    "        f\"{metadata['journal']}. {metadata['year']};\"\n",
    "        f\"{metadata['volume']}({metadata['issue']}):{metadata['pages']}.\"\n",
    "    )\n",
    "\n",
    "    if pmid:\n",
    "        citation += f\" PMID: {pmid}\"\n",
    "\n",
    "    return citation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5985ff11",
   "metadata": {},
   "outputs": [],
   "source": [
    "citation = format_ama_citation(pmdict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79709767",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Markdown(f\"# **Reanalysis of \\\"{pmdict['title']}\\\" by {pmdict['authors'][0]} et al., {pmdict['journal']}, {pmdict['year']}**\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "796adb48",
   "metadata": {},
   "outputs": [],
   "source": [
    "link = f\"https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc={gse}\"\n",
    "display(Markdown(f\"{citation}\"))\n",
    "HTML(f'<a href=\"{link}\" target=\"_blank\">Visit GEO accession page</a>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7579908",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google import genai\n",
    "\n",
    "os.environ[\"GOOGLE_API_KEY\"] = os.getenv(\"GOOGLE_API_KEY\")\n",
    "\n",
    "client = genai.Client(api_key=os.environ[\"GOOGLE_API_KEY\"])\n",
    "#print(os.environ[\"GOOGLE_API_KEY\"])\n",
    "prompt = f'''\n",
    "  You are an expert academic writer. Your task is to reformat the provided research information into a concise abstract around 250 words following this exact template:\n",
    "\n",
    "  \"In this study, <FIRST AUTHOR> et al. [1] profiled <CELLS AND CONDITIONS> to further our understanding of <TOPIC>. The reanalysis of this dataset include <FILL IN>\"\n",
    "\n",
    "  Here is the contextual information:\n",
    "  Author: {pmdict['authors']}\n",
    "  Title: {pmdict['title']}\n",
    "  Summary: {record[0]['summary']}\n",
    "\n",
    "  In the reanalysis explanation, use the following information: the reanalysis is a full RNA-seq analysis pipeline that consists of: UMAP[2], PCA[3], t-SNE[4] plots of the samples; clustergram heatmap; differential gene expression analysis\n",
    "  for each pair of control and perturbation samples; Enrichment analysis for each gene signature using Enrichr [5, 6, 7]; Transcription factor analysis of gene signatures\n",
    "  using ChEA3 [8] ; Reverser and mimicker drug match analysis using L2S2 [9] and DRUG-seqr [10], both FDA and non-FDA approved. Results are provided as tables in addition to bar charts.\n",
    "\n",
    "  Please write the reanalysis as a complete paragraph with smoothly transitioning sentences. Use consistent, present tense.\n",
    "  Do not omit or change the ordering of the reference numbers.\n",
    "  Do not change the reference and insert it, in parentheses, where indicated. \n",
    "\n",
    "  Now, generate the abstract strictly following the template. Do not include any other text or introductory/concluding remarks.\n",
    "'''\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=\"gemini-2.0-flash\", contents=prompt\n",
    ")\n",
    "#print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ab7b2b0",
   "metadata": {},
   "source": [
    "## **Abstract**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9094c5a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Markdown(f\"{response.text}\\n*This abstract was generated with the assistance of Gemini 2.0 Flash.*\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1091ef8",
   "metadata": {},
   "source": [
    "## **Methods**\n",
    "\n",
    "*RNA-seq alignment*\n",
    "\n",
    "Gene count matrices were obtained from ARCHS4 [11], which preprocessed the raw FASTQ data using the Kallisto [12] and STAR [] pseudoalignment algorithm.\n",
    "\n",
    "*Gene matrix processing* \n",
    "\n",
    "The raw gene matrix was filtered to remove genes that do not have an average of 3 reads across the samples. It was then quantile, log2, and z-score normalized. A regex-based function was used to infer whether individual samples belong to a “control” or a “perturbation” group by processing the metadata associated with each sample. \n",
    "\n",
    "*Dimensionality Reduction Visualization*\n",
    "\n",
    "Three types of dimensionality reduction techniques were applied on the processed expression matrices: UMAP[2], PCA[3], and t-SNE[4]. UMAP was calculated by the UMAP Python package and PCA and t-SNE were calculated using the Scikit-Learn Python library. The samples were then represented on 2D scatterplots.\n",
    "\n",
    "*Clustergram Heatmap*\n",
    "\n",
    "As a preliminary step, the top 1000 genes exhibiting most variability were selected. Using this new set, clustergram heatmaps were generated. Two versions of the clustergram exist: an interactive one generated by Clustergrammer [13] and a publication-ready alternative.\n",
    "\n",
    "*Differentially Expressed Genes Calculation and Volcano Plot*\n",
    "\n",
    "Differentially expressed genes between the control and perturbation samples were calculated using Limma Voom [14]. The logFC and -log10p values of each gene were visualized as a volcano scatterplot. Upregulated and downregulated genes were selected according to this criteria: p < 0.05 and |logFC| > 1.0.\n",
    "\n",
    "*Enrichr Enrichment Analysis*\n",
    "\n",
    "The upregulated and down-regulated sets were separately submitted to Enrichr [5, 6, 7]. These sets were compared against libraries from ChEA [8], ARCHS4 [12], Reactome Pathways [15], MGI Mammalian Phenotype [16], Gene Ontology Biological Processes [17], GWAS Catalog [18], KEGG [19, 20, 21], and WikiPathways [22]. The top matched terms from each library and their respective -log10p values were visualized as barplots.\n",
    "\n",
    "*Chea3 Transcription Factor Analysis*\n",
    "\n",
    "The upregulated and down-regulated sets were separately submitted to Chea3 [8]. These sets were compared against the libraries ARCHS4 Coexpression [12], GTEx Coexpression [23], Enrichr [5, 6, 7], ENCODE ChIP-seq [24, 25], ReMap ChIP-seq [26], and Literature-mined ChIP-seq. The top matched TFs were ranked according to their average score across each library and represented as barplots.\n",
    "\n",
    "*L2S2 and Drug-seqr drug analysis*\n",
    "\n",
    "The top 500 up and downregulated sets were submitted simulataneously to identify reverser and mimicker molecules, both FDA and non-FDA approved, from the L2S2 [9] and Drug-seqr [10] databases. The top matched molecules were compiled into tables and visualized as barplots. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bba549e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "#write a json file as a catalog list.\n",
    "metadata_path = Path(os.path.join(project_root, \"data\", \"metadata.json\"))\n",
    "\n",
    "# 1. Load existing metadata (or create empty if file doesn't exist)\n",
    "if metadata_path.exists():\n",
    "    with open(metadata_path, \"r\") as f:\n",
    "        metadata = json.load(f)\n",
    "else:\n",
    "    metadata = {}\n",
    "\n",
    "\n",
    "entry = {\n",
    "    \"GSE\": gse, \n",
    "    \"author\": \", \".join(pmdict['authors']),\n",
    "    \"year\": pmdict['year'],\n",
    "    \"species\": species,\n",
    "    \"title\": pmdict['title'],\n",
    "    \"pmid\": pmid,\n",
    "    \"num_samps\": len(record[0]['Samples']),\n",
    "    \"samples\": \", \".join(sorted([w['Accession'] for w in record[0]['Samples']])),\n",
    "    \"citation\": citation,\n",
    "    \"notebook_path\": f\"{resource_path}/{gse}.ipynb\",\n",
    "    #\"report_path\": f\"{resource_path}/{gse}.html\",\n",
    "    \"timestamp\": datetime.now().isoformat()\n",
    "}\n",
    "\n",
    "# 3. Add entry only if it doesn't already exist\n",
    "if gse not in metadata:\n",
    "    metadata[gse] = entry\n",
    "    print(f\"[INFO] Added metadata for {gse}\")\n",
    "else:\n",
    "    print(f\"[INFO] {gse} already exists in metadata. Skipping update.\")\n",
    "\n",
    "# 4. Write updated metadata back to file\n",
    "with open(metadata_path, \"w\") as f:\n",
    "    json.dump(metadata, f, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d115ce0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tab_num = 1\n",
    "fig_num = 1\n",
    "save_formats = ['png', 'svg', 'jpeg']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d05682c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import archs4py as a4\n",
    "#file_path = a4.download.counts(\"human\", path=\"\", version=\"latest\") #comment out if the file is already downloaded\n",
    "file = os.path.join(\"/home/ajy20/projects/8--auto-playbook-geo-reports\", \"human_gene_v2.latest.h5\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31f101e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = a4.meta.series(file, gse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4326791",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download('stopwords')\n",
    "words_to_remove = ['experiement', 'tissue', 'type', 'batch', 'treatment', 'experiment', 'patient', 'batch', '1', '2', '3', '4', '5', '6', '7', '8', '9']\n",
    "stopwords_plus = set(stopwords.words('english') + (words_to_remove))\n",
    "pattern = r'[-,_.:]'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e380c238",
   "metadata": {},
   "outputs": [],
   "source": [
    "terms_to_remove = [\"cell line\", \"cell type\", \"genotype\", \"treatment\"]\n",
    "\n",
    "\n",
    "pattern1 = r\"\\b(\" + \"|\".join(map(re.escape, terms_to_remove)) + r\")\\b\"\n",
    "\n",
    "metadata[\"cleaned_characteristics\"] = metadata[\"characteristics_ch1\"].str.replace(\n",
    "    pattern1, \n",
    "    \"\", \n",
    "    flags=re.IGNORECASE, \n",
    "    regex=True\n",
    ").str.replace(r\"\\s+\", \" \", regex=True).str.strip()\n",
    "\n",
    "metadata['cleaned_characteristics'] = metadata['cleaned_characteristics'].apply(lambda x: re.sub(pattern, \" \", x).strip().lower())\n",
    "metadata['cleaned_characteristics'] = metadata['cleaned_characteristics'].apply(\n",
    "    lambda text: \" \".join([word for word in text.split() if word not in stopwords_plus])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c78e306",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata['clean_title'] = metadata['title'].apply(lambda x: re.sub('[0-9]+', '', x))\n",
    "metadata['clean_title'] = metadata['clean_title'].apply(lambda x: re.sub(pattern, \" \", x).strip().lower())\n",
    "metadata['clean_title'] = metadata['clean_title'].apply(\n",
    "    lambda text: \" \".join([word for word in text.split() if word not in stopwords_plus])\n",
    ")\n",
    "\n",
    "#metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5429d180",
   "metadata": {},
   "outputs": [],
   "source": [
    "groups = metadata.groupby(by='clean_title', level=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb23d1a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "ctrl_words = set(['wt', 'wildtype', 'control', 'cntrl', 'ctrl', 'uninfected', 'normal', 'untreated', 'unstimulated', 'shctrl', 'ctl', 'healthy', 'sictrl', 'sicontrol', 'ctr', 'wild', 'dmso', 'vehicle', 'naive'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39be4c54",
   "metadata": {},
   "outputs": [],
   "source": [
    "groupings = {}\n",
    "for label, group in groups:\n",
    "    if len(group) not in {3, 4}: #enforce 3-4 samples per group\n",
    "        raise ValueError(\"Study does not have 3-4 samples per group\")\n",
    "    \n",
    "    groupings[label] = group['geo_accession'].tolist()\n",
    "\n",
    "# print(groupings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28892de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "title_conditions = list(groupings.keys())\n",
    "title_ctrl = []\n",
    "for c in title_conditions:\n",
    "    if len(set(c.split()).intersection(ctrl_words)) > 0:\n",
    "        title_ctrl.append(c)\n",
    "        \n",
    "# print(title_conditions)\n",
    "# print(title_ctrl)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "692d617b",
   "metadata": {},
   "outputs": [],
   "source": [
    "og_labels = {}\n",
    "labled_groupings = {}\n",
    "\n",
    "for label in groupings:\n",
    "    samps = groupings[label]\n",
    "    data = list(map(lambda s: s.lower(), metadata.loc[samps]['characteristics_ch1'].values))\n",
    "    data_clean = []\n",
    "    for d in data:\n",
    "        data_clean.append(set(filter(lambda w: w not in stopwords_plus, re.sub(pattern, ' ', d).split())))\n",
    "    condition = set(data_clean[0])\n",
    "    for s in data_clean[1:]:\n",
    "        condition.intersection_update(s)\n",
    "    condition = ' '.join(list(condition))\n",
    "    labled_groupings[condition] = samps\n",
    "    og_labels[condition] = label\n",
    "\n",
    "ch1_ctrl = []\n",
    "ch1_conditions = list(labled_groupings.keys())\n",
    "\n",
    "for condition in labled_groupings:\n",
    "    split_conditions = condition.lower().split()\n",
    "    if len(set(split_conditions).intersection(ctrl_words)) > 0:\n",
    "        ch1_ctrl.append(condition)\n",
    "\n",
    "# print(og_labels)\n",
    "# print(ch1_conditions)\n",
    "# print(ch1_ctrl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d123545",
   "metadata": {},
   "outputs": [],
   "source": [
    "#must have 1-2 controls. Must have perturbation groups as well (not all groups can be controls).\n",
    "def check_eligibility(conditions, ctrl_conditions):\n",
    "    if len(ctrl_conditions) not in range(1, 3) or len(ctrl_conditions) == len(conditions):\n",
    "        return False\n",
    "    else:\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49301d17",
   "metadata": {},
   "outputs": [],
   "source": [
    "ch1_eligibility = check_eligibility(ch1_conditions, ch1_ctrl)\n",
    "title_eligibility = check_eligibility(title_conditions, title_ctrl)\n",
    "#print(ch1_eligibility)\n",
    "#rint(title_eligibility)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6577b68",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_groups(title_ctrl, ch1_ctrl, og_labels):\n",
    "    #convert ch1 condition to corresponding title condition, check if their respective sample sets are equal\n",
    "    for c in ch1_ctrl:\n",
    "        ch1_corresponding = og_labels[c]\n",
    "        if set(groupings[ch1_corresponding]) != set(labled_groupings[c]):\n",
    "            return False\n",
    "\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81c840df",
   "metadata": {},
   "outputs": [],
   "source": [
    "if ch1_eligibility and title_eligibility:\n",
    "    if compare_groups(title_ctrl, ch1_ctrl, og_labels):\n",
    "        ctrl_conditions = title_ctrl\n",
    "        conditions = title_conditions\n",
    "    else:\n",
    "        raise Exception(\"Group Assignment Failed\")\n",
    "    \n",
    "elif ch1_eligibility ^ title_eligibility:\n",
    "    if ch1_eligibility:\n",
    "        ctrl_conditions = ch1_ctrl\n",
    "        conditions = ch1_conditions\n",
    "        groupings = labled_groupings\n",
    "    else:\n",
    "        ctrl_conditions = title_ctrl\n",
    "        conditions = title_conditions\n",
    "else:\n",
    "    raise Exception(\"Group Assignment Failed\")\n",
    "\n",
    "# print(ctrl_conditions)\n",
    "# print(conditions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae8370bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "gene_matrix = a4.data.series(file, gse) #raw counts\n",
    "gene_matrix.to_csv(os.path.join(resource_path, \"raw_counts.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afbe5209",
   "metadata": {},
   "outputs": [],
   "source": [
    "gene_matrix.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c872f58",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Markdown(f\"**table {tab_num}**: This is a preview of the first 5 rows of the raw RNA-seq expression matrix from {gse}.\"))\n",
    "tab_num +=1\n",
    "display(FileLink(os.path.join(resource_path, \"raw_counts.csv\"), result_html_prefix=\"Download raw counts: \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4df8d119",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove genes with all-zero counts\n",
    "filtered_matrix = gene_matrix.loc[gene_matrix.sum(axis=1) > 0, :]\n",
    "\n",
    "# Then filter out low average expression\n",
    "filtered_matrix = filtered_matrix.loc[filtered_matrix.mean(axis=1) >= 3, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c03fa065",
   "metadata": {},
   "outputs": [],
   "source": [
    "from maayanlab_bioinformatics.normalization.log import log2_normalize\n",
    "from maayanlab_bioinformatics.normalization.zscore import zscore_normalize \n",
    "from maayanlab_bioinformatics.normalization.quantile_legacy import quantile_normalize\n",
    "\n",
    "def normalize(gene_counts):\n",
    "    norm_exp = quantile_normalize(gene_counts)\n",
    "    norm_exp = log2_normalize(norm_exp)\n",
    "    norm_exp = zscore_normalize(norm_exp)\n",
    "    return norm_exp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59dd0900",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# from python_scripts.matrix import normalize\n",
    "norm_matrix = normalize(filtered_matrix) #normalize the matrix for dim reduction and clustergram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe8da99d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def annotate_matrix(expr_df, groupings, ctrl_conditions):\n",
    "    sampdict = {}\n",
    "    for group in groupings.keys():\n",
    "        samps = groupings[group]\n",
    "        for samp in samps:\n",
    "            if group in ctrl_conditions:\n",
    "                sampdict[samp] = \"control\"\n",
    "            else:\n",
    "                sampdict[samp] = \"perturbation\"\n",
    "    \n",
    "    annotat = pd.DataFrame.from_dict(sampdict, orient='index', columns=['group'])\n",
    "    anndict = {\n",
    "        'count': expr_df,\n",
    "        'annotations': annotat\n",
    "    }\n",
    "    return anndict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d56663cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "annotated_norm_matrix = annotate_matrix(norm_matrix, groupings, ctrl_conditions)\n",
    "\n",
    "#print(annotated_norm_matrix['annotations'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac29639a",
   "metadata": {},
   "outputs": [],
   "source": [
    "annotated_matrix = annotate_matrix(filtered_matrix.astype('int64'), groupings, ctrl_conditions) #filtered but not normalized"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e4ab1ed",
   "metadata": {},
   "source": [
    "# **Results**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1262abd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_html = True #if true, render plotly graphs as html and embed with ipython. else, use fig.show()\n",
    "use_fig_plot = False #if true, render matplotlib graphs using show(), else it will render the saved pngs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d4e33a4",
   "metadata": {},
   "source": [
    "## **Dimensionality Reduction**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d8d6031",
   "metadata": {},
   "source": [
    "### **UMAP**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad3b5e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import python_scripts.visualizations as vis\n",
    "\n",
    "vis.plot(annotated_norm_matrix['count'], annotated_norm_matrix['annotations'], n_components=2, save_formats=save_formats, decomp=\"umap\", save_html=save_html, save_path=resource_path)\n",
    "# if save_html: display(HTML(os.path.join(resource_path, \"umap.html\")))\n",
    "display(Image(os.path.join(resource_path, \"umap.png\"), width=700))\n",
    "display(Markdown(f\"**Figure {fig_num}**: This figure displays a 2D scatter plot of a UMAP decomposition of the sample data. Each point represents an individual sample, colored by its experimental group.\"))\n",
    "fig_num+=1\n",
    "\n",
    "for fmt in save_formats:\n",
    "    display(FileLink(os.path.join(resource_path, f\"umap.{fmt}\"), result_html_prefix=f\"Download UMAP figure as {fmt}: \"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf3a6b8f",
   "metadata": {},
   "source": [
    "## **Clustergram Heatmaps**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71b3bce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from maayanlab_bioinformatics.normalization.filter import filter_by_var\n",
    "norm_t1000 = annotated_norm_matrix['count'].copy()\n",
    "norm_t1000 = filter_by_var(annotated_norm_matrix['count'], top_n=1000, axis=1)\n",
    "norm_t1000.columns=metadata['title'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd917d87",
   "metadata": {},
   "outputs": [],
   "source": [
    "t1000_path = os.path.join(resource_path, 'expression_matrix_top1000_genes.txt')\n",
    "norm_t1000.to_csv(t1000_path, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25db3543",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests, json\n",
    "clustergrammer_url = 'http://amp.pharm.mssm.edu/clustergrammer/matrix_upload/'\n",
    "\n",
    "r = requests.post(clustergrammer_url, files={'file': open(t1000_path, 'rb')})\n",
    "link = r.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3dd8308",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import IFrame\n",
    "display(IFrame(link, width=\"600\", height=\"650\"))\n",
    "display(Markdown(f\"**Figure {fig_num}**: The figure contains an interactive heatmap displaying gene expression for each sample in the RNA-seq dataset. Every row of the heatmap represents a gene, every column represents a sample, and every cell displays normalized gene expression values. The heatmap additionally features color bars beside each column which represent prior knowledge of each sample, such as the tissue of origin or experimental treatment.\"))\n",
    "fig_num+=1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
